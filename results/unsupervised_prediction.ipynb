{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to evaluate anomaly scores by unsupervised prediction performance\n",
    "\n",
    "To run this notebook you need to create and activate the following conda environment:\n",
    "\n",
    "```\n",
    "conda create --name score_eval -c conda-forge -c defaults numpy pandas matplotlib seaborn scipy scikit-learn ipython ipykernel -y\n",
    "conda activate score_eval\n",
    "pip install -e .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils_eval_score import (\n",
    "    display_scatterplot_w_scores,\n",
    "    get_scores_n_abx_info,\n",
    ")\n",
    "from src.utils_prediction import calculate_metrics, plot_distribution\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER input: define the inferred model and linked datasets to evaluate here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT START\n",
    "# name of the model\n",
    "model_name = \"saved_models_microbial_novel_alpha_div2/id-55\"\n",
    "# which model version to evaluate: \"best\" or \"last\"\n",
    "point_to_evaluate = \"best\"\n",
    "\n",
    "# name of feature dataset used for model\n",
    "ft_name = \"ft_vat19_anomaly_v20240806_entero_genus\"\n",
    "# path to abx time-series file\n",
    "path_to_abx_data = \"../data/original_data/\"\n",
    "# name of abx time-series used for model\n",
    "abx_ts_name = \"ts_vat19_abx_v20240806\"\n",
    "\n",
    "# limit evaluation to time range up to this many months (if None no limit is set\n",
    "# and all scores are evaluated)\n",
    "limit_months = 24.0\n",
    "\n",
    "# scaling factor options:\n",
    "scaling_factors_used = False\n",
    "\n",
    "# if scaling_factors_used is True, then the following options are required:\n",
    "# non-centered = \"nc_std\" or centered = \"std\"\n",
    "stddev_type = \"std\"\n",
    "# moving average window size: 30 or 10\n",
    "moving_avg = 10\n",
    "# whether to include duplicates: \"--RD-True\" or \"\"\n",
    "duplicates = \"--RD-True\"\n",
    "\n",
    "#### USER INPUT END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f\"../data/{model_name}/anomaly_detection/\"\n",
    "\n",
    "if scaling_factors_used:\n",
    "    print(\"Scaling factors used.\")\n",
    "    folder_name = f\"using-SF_{stddev_type}_z_scores--moving_avg-{moving_avg}-cummax-lower_bound-1{duplicates}\"\n",
    "\n",
    "    scores_path = f\"{base_path}scores_{point_to_evaluate}_normal/{folder_name}/\"\n",
    "    evaluation_path = f\"{base_path}evaluation_{point_to_evaluate}_unsupervised_pred_{stddev_type}_ma{moving_avg}{duplicates.replace(\"-\", \"_\").lower()}/\"\n",
    "else:\n",
    "    scores_path = f\"{base_path}scores_{point_to_evaluate}_normal/\"\n",
    "    evaluation_path = f\"{base_path}evaluation_{point_to_evaluate}_unsupervised_pred/\"\n",
    "\n",
    "if not os.path.exists(evaluation_path):\n",
    "    os.makedirs(evaluation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores\n",
    "noabx_train, noabx_val, abx_scores_flat, abx_df, abx_age_at_all = get_scores_n_abx_info(\n",
    "    scores_path, ft_name, limit_months, abx_ts_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noabx_train[[\"score_0\", \"score_1\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abx_scores_flat[[\"score_0\", \"score_1\", \"score_2\", \"score_3\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score_0 vs score_1 for abx cohort\n",
    "plot_distribution(\n",
    "    columns=[\"score_0\", \"score_1\"],\n",
    "    dataframes={\"abx_scores_flat\": abx_scores_flat},\n",
    "    figsize=(4, 5),\n",
    "    kde=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score_0 for abx vs. noabx train cohort\n",
    "plot_distribution(\n",
    "    columns=\"score_0\",\n",
    "    dataframes={\n",
    "        \"abx_scores_flat\": abx_scores_flat,\n",
    "        \"noabx_train\": noabx_train,\n",
    "        \"noabx_val\": noabx_val,\n",
    "    },\n",
    "    figsize=(10, 6),\n",
    "    kde=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort both abx dataframes by increasing abx exposure in same way\n",
    "abx_scores_flat.sort_values(\n",
    "    [\n",
    "        \"abx_max_count_ever\",\n",
    "        \"max_abx_w_microbiome\",\n",
    "        \"host_id\",\n",
    "        \"day\",\n",
    "    ],\n",
    "    ascending=[True, True, True, True],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# sort abx_df accordingly\n",
    "# sort abx_df in same order and remove samples that don't exist in md_df\n",
    "abx_events = pd.DataFrame()\n",
    "abx_events[\"host_id\"] = abx_scores_flat[\"host_id\"].unique()\n",
    "abx_events = pd.merge(abx_events, abx_df, on=\"host_id\", how=\"left\")\n",
    "assert abx_events.host_id.unique().tolist() == abx_scores_flat.host_id.unique().tolist()\n",
    "\n",
    "# display scatter\n",
    "dic_splits = {\n",
    "    \"train_noabx\": [\"score_0\", noabx_train, None],\n",
    "    \"val_noabx\": [\"score_0\", noabx_val, None],\n",
    "    \"abx\": [\"score_0\", abx_scores_flat, abx_events],\n",
    "}\n",
    "\n",
    "display_scatterplot_w_scores(\n",
    "    dic_splits, False, path_to_output=evaluation_path, flag=\"noabx_vs_abx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer threshold for noabx exposed score from noabx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noabx_val.score_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = noabx_val[\"score_0\"].quantile(0.9)\n",
    "thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance of inferred threshold\n",
    "\n",
    "goal: detect observed samples in first 1 month after 1st abx exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()\n",
    "df_results.index.name = \"quantile\"\n",
    "for q in reversed([0.7, 0.8, 0.9, 0.95, 0.97, 0.99]):\n",
    "    print(f\"Quantile: {q}\")\n",
    "    thresh = noabx_val[\"score_0\"].quantile(q)\n",
    "    th_sample_after_abx_months = 1.0\n",
    "\n",
    "    abx_scores_flat_th = abx_scores_flat.copy()\n",
    "\n",
    "    # flag samples that were observed within x months after abx exposure\n",
    "    abx_scores_flat_th = abx_scores_flat_th.assign(\n",
    "        sample_lt_xm_after_abx=lambda df: df[\"abx_any_last_t_dmonths\"]\n",
    "        <= th_sample_after_abx_months\n",
    "    )\n",
    "\n",
    "    # add target description: sample lt x months after abx & cumcount == 1\n",
    "    abx_scores_flat_th = abx_scores_flat_th.assign(\n",
    "        true_target=lambda df: df[\"sample_lt_xm_after_abx\"]\n",
    "        & (df[\"abx_any_cumcount\"] == 1)\n",
    "    )\n",
    "    print(abx_scores_flat_th.true_target.value_counts(dropna=False))\n",
    "    print()\n",
    "\n",
    "    # define predicted target\n",
    "    abx_scores_flat_th[\"pred_target\"] = abx_scores_flat_th[\"score_0\"] > thresh\n",
    "\n",
    "    # evaluate classification\n",
    "    report = calculate_metrics(\n",
    "        abx_scores_flat_th[\"true_target\"], abx_scores_flat_th[\"pred_target\"]\n",
    "    )\n",
    "\n",
    "    df_results.loc[q, \"true_f1_score\"] = report[\"True\"][\"f1-score\"]\n",
    "    df_results.loc[q, \"macro_avg_f1_score\"] = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
