{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to evaluate anomaly scores by unsupervised prediction performance\n",
    "\n",
    "To run this notebook you need to create and activate the following conda environment:\n",
    "\n",
    "```\n",
    "conda create --name score_eval -c conda-forge -c defaults numpy pandas matplotlib seaborn scipy scikit-learn ipython ipykernel -y\n",
    "conda activate score_eval\n",
    "pip install -e .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "\n",
    "from src.utils_eval_score import (\n",
    "    _add_month_bins,\n",
    "    _get_abx_info,\n",
    "    _transform_scores,\n",
    "    display_scatterplot_w_scores,\n",
    "    get_scores_n_abx_info,\n",
    ")\n",
    "from src.utils_prediction import calculate_metrics\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER input: define the inferred model and linked datasets to evaluate here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT START\n",
    "# name of the model\n",
    "model_name = \"saved_models_microbial_novel_alpha_div2/id-55\"\n",
    "# which model version to evaluate: \"best\" or \"last\"\n",
    "point_to_evaluate = \"best\"\n",
    "\n",
    "# name of feature dataset used for model\n",
    "ft_name = \"ft_vat19_anomaly_v20240806_entero_genus\"\n",
    "# path to abx time-series file\n",
    "path_to_abx_data = \"../data/original_data/\"\n",
    "# name of abx time-series used for model\n",
    "abx_ts_name = \"ts_vat19_abx_v20240806\"\n",
    "\n",
    "# limit evaluation to time range up to this many months (if None no limit is set\n",
    "# and all scores are evaluated)\n",
    "limit_months = 24.0\n",
    "#### USER INPUT END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_no_cutoff_scores(scores_path, split, limit_months=None):\n",
    "    scores = pd.read_csv(f\"{scores_path}{split}_ad_scores_False_coord-0.csv\")\n",
    "\n",
    "    scores_t = _transform_scores(scores)\n",
    "    scores_t = _add_month_bins(scores_t)\n",
    "\n",
    "    if limit_months is not None:\n",
    "        scores_t = scores_t[scores_t[\"month5_bin\"] <= limit_months].copy()\n",
    "    return scores_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_path = (\n",
    "    f\"../data/{model_name}/anomaly_detection/scores_{point_to_evaluate}_normal/\"\n",
    ")\n",
    "evaluation_path = f\"../data/{model_name}/anomaly_detection/evaluation_{point_to_evaluate}_unsupervised_pred/\"\n",
    "\n",
    "if not os.path.exists(evaluation_path):\n",
    "    os.makedirs(evaluation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scores\n",
    "noabx_train, noabx_val, abx_scores_flat, abx_df, abx_age_at_all = get_scores_n_abx_info(\n",
    "    scores_path, ft_name, limit_months, abx_ts_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abx_scores_flat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort both abx dataframes by increasing abx exposure in same way\n",
    "abx_scores_flat.sort_values(\n",
    "    [\n",
    "        \"abx_max_count_ever\",\n",
    "        \"max_abx_w_microbiome\",\n",
    "        \"host_id\",\n",
    "        \"day\",\n",
    "    ],\n",
    "    ascending=[True, True, True, True],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# sort abx_df accordingly\n",
    "# sort abx_df in same order and remove samples that don't exist in md_df\n",
    "abx_events = pd.DataFrame()\n",
    "abx_events[\"host_id\"] = abx_scores_flat[\"host_id\"].unique()\n",
    "abx_events = pd.merge(abx_events, abx_df, on=\"host_id\", how=\"left\")\n",
    "assert abx_events.host_id.unique().tolist() == abx_scores_flat.host_id.unique().tolist()\n",
    "\n",
    "# display scatter\n",
    "dic_splits = {\n",
    "    \"train_noabx\": [\"score_0\", noabx_train, None],\n",
    "    \"val_noabx\": [\"score_0\", noabx_val, None],\n",
    "    \"abx\": [\"score_0\", abx_scores_flat, abx_events],\n",
    "}\n",
    "\n",
    "display_scatterplot_w_scores(\n",
    "    dic_splits, False, path_to_output=evaluation_path, flag=\"noabx_vs_abx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "open points:\n",
    "* ensure score_0 was also scaled as it should"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer threshold for noabx exposed score from noabx_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noabx_val.score_0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = noabx_val[\"score_0\"].quantile(0.95)\n",
    "thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| q-threshold | 3M true_f1_score | 3M weighted_avg_f1_score | 3M accuracy |\n",
    "|-------------|------------------|--------------------------|-------------|\n",
    "| 0.7         | 0.21             | 0.66                     | 0.58        |\n",
    "| 0.8         | 0.20             | 0.73                     | 0.68        |\n",
    "| 0.9         | 0.19             | 0.80                     | 0.79        |\n",
    "| 0.95        | 0.13             | 0.81                     | 0.82        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate performance of inferred threshold\n",
    "\n",
    "goal: detect observed samples in first 1,2,3 months after 1st abx exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th_sample_after_abx_months in reversed(range(1,4)):\n",
    "    print(th_sample_after_abx_months)\n",
    "    abx_scores_flat_th = abx_scores_flat.copy()\n",
    "\n",
    "    # flag samples that were observed within x months after abx exposure\n",
    "    abx_scores_flat_th = abx_scores_flat_th.assign(\n",
    "        sample_lt_xm_after_abx=lambda df: df[\"abx_any_last_t_dmonths\"]\n",
    "        <= th_sample_after_abx_months\n",
    "    )\n",
    "\n",
    "    # add target description: sample lt x months after abx & cumcount == 1\n",
    "    abx_scores_flat_th = abx_scores_flat_th.assign(\n",
    "        true_target=lambda df: df[\"sample_lt_xm_after_abx\"] & (df[\"abx_any_cumcount\"] == 1)\n",
    "    )\n",
    "    print(abx_scores_flat_th.true_target.value_counts(dropna=False))\n",
    "    print()\n",
    "    # define predicted target\n",
    "    abx_scores_flat_th[\"pred_target\"] = abx_scores_flat_th[\"score_0\"] > thresh\n",
    "\n",
    "    # evaluate classification \n",
    "    calculate_metrics(abx_scores_flat_th[\"true_target\"], abx_scores_flat_th[\"pred_target\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
