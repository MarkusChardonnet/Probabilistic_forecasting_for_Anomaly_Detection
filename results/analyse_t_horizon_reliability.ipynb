{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to infer reliable time horizon\n",
    "\n",
    "To run this notebook you need to create and activate the following conda environment:\n",
    "\n",
    "```\n",
    "conda create --name score_eval -c conda-forge -c defaults numpy pandas matplotlib seaborn scipy scikit-learn ipython ipykernel -y\n",
    "conda activate score_eval\n",
    "pip install -e .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils_eval_score import (\n",
    "    _get_ordinal_suffix,\n",
    "    _get_step_n_indicator,\n",
    "    _group_samples_prior_to_cutoff,\n",
    "    _plot_score_after_nth_abx_exposure,\n",
    "    get_cutoff_value_sample_sizes,\n",
    "    get_scores_n_abx_info,\n",
    ")\n",
    "from src.utils_t_horizon import (\n",
    "    display_two_distributions,\n",
    "    enrich_scores,\n",
    "    plot_cutoff_date_distribution,\n",
    "    sample_from_each_group,\n",
    "    select_last_score_per_host_per_bin,\n",
    "    transform_cutoff_scores,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# avg. number of days per month\n",
    "DAYS_PER_MONTH = 30.437"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER input: define the inferred model and linked datasets to evaluate here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT START\n",
    "# name of the model\n",
    "model_name = \"saved_models_microbial_novel_alpha_div2/id-55\"\n",
    "# which model version to evaluate: \"best\" or \"last\"\n",
    "point_to_evaluate = \"best\"\n",
    "\n",
    "# name of feature dataset used for model\n",
    "ft_name = \"ft_vat19_anomaly_v20240806_entero_genus\"\n",
    "# name of abx time-series used for model\n",
    "abx_ts_name = \"ts_vat19_abx_v20240806\"\n",
    "\n",
    "# limit evaluation to time range up to this many months (if None no limit is set\n",
    "# and all scores are evaluated)\n",
    "limit_months = 24.0\n",
    "\n",
    "# whether to group samples prior to cutoff in analysis\n",
    "group_samples = True\n",
    "\n",
    "# how many samples prior and after cutoff to consider\n",
    "min_samples = -3.0\n",
    "max_samples = 12.0\n",
    "\n",
    "# whether to filter noabx score samples by having at least 1 obs prior to cutoff\n",
    "no_filter = True\n",
    "\n",
    "# whether to have max. resolution of 0.5 months or not\n",
    "max_resolution = False\n",
    "\n",
    "# scaling factor options:\n",
    "scaling_factors_used = True\n",
    "\n",
    "# if scaling_factors_used is True, then the following options are required:\n",
    "# non-centered = \"nc_std\" or centered = \"std\"\n",
    "stddev_type = \"nc_std\"\n",
    "# moving average window size: 30 or 10\n",
    "moving_avg = 10\n",
    "# whether to include duplicates: \"--RD-True\" or \"\"\n",
    "duplicates = \"--RD-False\"\n",
    "# using lower bound of 1 for SFs: \"lower_bound-1\" or \"\"\n",
    "lower_bound = \"\"\n",
    "\n",
    "#### USER INPUT END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling factors used.\n"
     ]
    }
   ],
   "source": [
    "base_path = f\"../data/{model_name}/anomaly_detection/\"\n",
    "\n",
    "res_n_group = f\"g{str(group_samples)[0]}_maxres{str(max_resolution)[0]}\"\n",
    "\n",
    "if scaling_factors_used:\n",
    "    print(\"Scaling factors used.\")\n",
    "    folder_name = f\"using-SF_{stddev_type}_z_scores--moving_avg-{moving_avg}-cummax{lower_bound}{duplicates}\"\n",
    "\n",
    "    cutoff_scores_path = (\n",
    "        f\"{base_path}reliability_eval-val-noabx_best_normal/{folder_name}/\"\n",
    "    )\n",
    "    evaluation_path = f\"{base_path}reliability_evaluation/{res_n_group}_{stddev_type}_ma{moving_avg}{duplicates.replace('-', '_').lower()}/\"\n",
    "\n",
    "    abx_scores_path = f\"{base_path}scores_{point_to_evaluate}_normal/{folder_name}/\"\n",
    "else:\n",
    "    print(\"No scaling factors used.\")\n",
    "    cutoff_scores_path = f\"{base_path}reliability_eval-val-noabx_best_normal/\"\n",
    "    evaluation_path = f\"{base_path}reliability_evaluation/{res_n_group}_no_scaling/\"\n",
    "    abx_scores_path = f\"{base_path}scores_{point_to_evaluate}_normal/\"\n",
    "\n",
    "if not os.path.exists(evaluation_path):\n",
    "    os.makedirs(evaluation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get abx scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/saved_models_microbial_novel_alpha_div2/id-55/anomaly_detection/scores_best_normal/using-SF_nc_std_z_scores--moving_avg-10-cummax--RD-False/train_ad_scores_1_coord-0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m noabx_train, noabx_val, abx_scores_flat, abx_df, abx_age_at_all \u001b[38;5;241m=\u001b[39m \u001b[43mget_scores_n_abx_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mabx_scores_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mft_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_months\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabx_ts_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_filter\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\Probabilistic_forecasting_for_Anomaly_Detection\\src\\utils_eval_score.py:963\u001b[0m, in \u001b[0;36mget_scores_n_abx_info\u001b[1;34m(scores_path, ft_name, limit_months, abx_ts_name, no_filter, path_to_data)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Processes scores and abx info for evaluation\"\"\"\u001b[39;00m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;66;03m# get train & val scores\u001b[39;00m\n\u001b[1;32m--> 963\u001b[0m scores_train \u001b[38;5;241m=\u001b[39m \u001b[43m_get_all_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_months\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_months\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m scores_val \u001b[38;5;241m=\u001b[39m _get_all_scores(scores_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, limit_months\u001b[38;5;241m=\u001b[39mlimit_months)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;66;03m# get noabx samples per split\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\Probabilistic_forecasting_for_Anomaly_Detection\\src\\utils_eval_score.py:48\u001b[0m, in \u001b[0;36m_get_all_scores\u001b[1;34m(path_to_scores, split, limit_months)\u001b[0m\n\u001b[0;32m     46\u001b[0m score_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)) \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m score_types:\n\u001b[1;32m---> 48\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpath_to_scores\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msplit\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_ad_scores_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_coord-0.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     50\u001b[0m scores_list \u001b[38;5;241m=\u001b[39m [_transform_scores(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m scores]\n\u001b[0;32m     52\u001b[0m scores_all \u001b[38;5;241m=\u001b[39m scores_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\marku\\.conda\\envs\\score_eval\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marku\\.conda\\envs\\score_eval\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\marku\\.conda\\envs\\score_eval\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marku\\.conda\\envs\\score_eval\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\marku\\.conda\\envs\\score_eval\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/saved_models_microbial_novel_alpha_div2/id-55/anomaly_detection/scores_best_normal/using-SF_nc_std_z_scores--moving_avg-10-cummax--RD-False/train_ad_scores_1_coord-0.csv'"
     ]
    }
   ],
   "source": [
    "noabx_train, noabx_val, abx_scores_flat, abx_df, abx_age_at_all = get_scores_n_abx_info(\n",
    "    abx_scores_path, ft_name, limit_months, abx_ts_name, no_filter=no_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get noabx cutoff scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noabx cutoff scores\n",
    "c_scores_list = []\n",
    "i_values = list(range(0, 1141, 30))\n",
    "\n",
    "for i in i_values:\n",
    "    c_scores = pd.read_csv(f\"{cutoff_scores_path}val_noabx_ad_scores_{i}_coord-0.csv\")\n",
    "\n",
    "    # transform scores from wide to long format\n",
    "    c_scores_t = transform_cutoff_scores(c_scores, DAYS_PER_MONTH)\n",
    "\n",
    "    # filter scores\n",
    "    c_scores_t = enrich_scores(c_scores_t, max_resolution=max_resolution)\n",
    "\n",
    "    # append each cutoff date to each other\n",
    "    c_scores_list.append(c_scores_t)\n",
    "\n",
    "c_scores_all = pd.concat(c_scores_list, ignore_index=True)\n",
    "\n",
    "if limit_months is not None:\n",
    "    c_scores_all = c_scores_all[\n",
    "        np.logical_and(\n",
    "            c_scores_all[\"month5_bin\"] <= limit_months,\n",
    "            c_scores_all[\"cutoff_month\"] < limit_months,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "c_scores_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot before filtering:\n",
    "plot_cutoff_date_distribution(c_scores_all, \"- before filtering\")\n",
    "\n",
    "# filter scores\n",
    "if not no_filter:\n",
    "    # filter: at least one observation must be present before the cutoff\n",
    "    c_scores_all_f = c_scores_all[c_scores_all[\"nb_obs_before_cutoff\"] > 0].copy()\n",
    "    # plot after filtering\n",
    "    plot_cutoff_date_distribution(c_scores_all_f, \"- after filtering\")\n",
    "else:\n",
    "    c_scores_all_f = c_scores_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by months_since_cutoff to be in min_ max_samples range\n",
    "c_scores_subset_f = c_scores_all_f.loc[\n",
    "    np.logical_and(\n",
    "        c_scores_all_f[\"months_since_cutoff\"] >= min_samples,\n",
    "        c_scores_all_f[\"months_since_cutoff\"] <= max_samples,\n",
    "    ),\n",
    "    :,\n",
    "]\n",
    "\n",
    "# bin scores if not max_resolution\n",
    "if not max_resolution:\n",
    "    c_scores_subset_f = c_scores_subset_f.copy()\n",
    "    # bin months_since_cutoff\n",
    "    bins = np.arange(min_samples, max_samples + 1.0, 1.0)\n",
    "    # Use the left edges of the bins as float labels\n",
    "    labels = bins[:-1]\n",
    "    c_scores_subset_f.loc[:, \"months_since_cutoff\"] = pd.cut(\n",
    "        c_scores_subset_f[\"months_since_cutoff\"], bins=bins, labels=labels, right=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all cutoffs grouped by cutoff value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cutoffs = c_scores_subset_f.use_obs_until_day.unique().tolist()\n",
    "\n",
    "# for cutoff in all_cutoffs:\n",
    "#     c_scores_subset_f_ss = c_scores_subset_f[\n",
    "#         c_scores_subset_f[\"use_obs_until_day\"] == cutoff\n",
    "#     ].copy()\n",
    "#     print(cutoff)\n",
    "\n",
    "#     # avoids having multiple scores per host per bin\n",
    "#     c_scores_subset_f_ss = select_last_score_per_host_per_bin(c_scores_subset_f_ss)\n",
    "\n",
    "#     # group samples prior to cutoff if needed - performed here to ensure same\n",
    "#     # seed sampling independent on group_samples value\n",
    "#     if group_samples:\n",
    "#         group_step, last_bin_indicator = _get_step_n_indicator(max_resolution)\n",
    "#         # uniqueness not based on only host_id here - but also on cutoff_month\n",
    "#         c_scores_subset_f_ss = _group_samples_prior_to_cutoff(\n",
    "#             c_scores_subset_f_ss,\n",
    "#             \"months_since_cutoff\",\n",
    "#             [\"host_id\", \"cutoff_month\"],\n",
    "#             min_samples,\n",
    "#             group_step,\n",
    "#             last_bin_indicator,\n",
    "#         )\n",
    "\n",
    "#     _plot_score_after_nth_abx_exposure(\n",
    "#         c_scores_subset_f_ss,\n",
    "#         x_axis=\"months_since_cutoff\",\n",
    "#         y_axis=\"score\",\n",
    "#         n=0,\n",
    "#         path_to_save=evaluation_path,\n",
    "#         flag=f\"noabx_cutoff{int(cutoff)}_scores\",\n",
    "#         tag=f\"with cutoff={cutoff}\",\n",
    "#         min_samples=min_samples,\n",
    "#         max_samples=max_samples,\n",
    "#         max_resolution=max_resolution,\n",
    "#         grouped_samples=group_samples,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all cutoffs pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoids having multiple scores per host per bin\n",
    "c_scores_subset_f = select_last_score_per_host_per_bin(c_scores_subset_f)\n",
    "\n",
    "# group samples prior to cutoff if needed - performed here to ensure same\n",
    "# seed sampling independent on group_samples value\n",
    "if group_samples:\n",
    "    group_step, last_bin_indicator = _get_step_n_indicator(max_resolution)\n",
    "    # uniqueness not based on only host_id here - but also on cutoff_month\n",
    "    c_scores_subset_f = _group_samples_prior_to_cutoff(\n",
    "        c_scores_subset_f,\n",
    "        \"months_since_cutoff\",\n",
    "        [\"host_id\", \"cutoff_month\"],\n",
    "        min_samples,\n",
    "        group_step,\n",
    "        last_bin_indicator,\n",
    "    )\n",
    "\n",
    "\n",
    "_plot_score_after_nth_abx_exposure(\n",
    "    c_scores_subset_f,\n",
    "    x_axis=\"months_since_cutoff\",\n",
    "    y_axis=\"score\",\n",
    "    n=0,\n",
    "    path_to_save=evaluation_path,\n",
    "    flag=\"noabx_cutoff_all_scores\",\n",
    "    tag=\"\",\n",
    "    min_samples=min_samples,\n",
    "    max_samples=max_samples,\n",
    "    max_resolution=max_resolution,\n",
    "    grouped_samples=group_samples,\n",
    "    uniqueness_var_ls=[\"use_obs_until_day\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize cutoffs - same \"cutoff\" subset as for 1st/2nd/3rd abx scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max_samples to 6 for consistency with abx comparison in evaluate_scores\n",
    "max_samples = 6.0\n",
    "\n",
    "# filter by months_since_cutoff to be in min_ max_samples range\n",
    "c_scores_subset_f = c_scores_all_f.loc[\n",
    "    np.logical_and(\n",
    "        c_scores_all_f[\"months_since_cutoff\"] >= min_samples,\n",
    "        c_scores_all_f[\"months_since_cutoff\"] <= max_samples,\n",
    "    ),\n",
    "    :,\n",
    "]\n",
    "\n",
    "# bin scores if not max_resolution\n",
    "if not max_resolution:\n",
    "    c_scores_subset_f = c_scores_subset_f.copy()\n",
    "    # bin months_since_cutoff\n",
    "    bins = np.arange(min_samples, max_samples + 1.0, 1.0)\n",
    "    # Use the left edges of the bins as float labels\n",
    "    labels = bins[:-1]\n",
    "    c_scores_subset_f.loc[:, \"months_since_cutoff\"] = pd.cut(\n",
    "        c_scores_subset_f[\"months_since_cutoff\"], bins=bins, labels=labels, right=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all host's cutoffs to choose from\n",
    "\n",
    "# # to only take hosts that have a value after cutoff\n",
    "# c_scores_subset_f_f = c_scores_subset_f[c_scores_subset_f[\"months_since_cutoff\"]==0.0].copy()\n",
    "# cutoff_host_mapping = c_scores_subset_f_f[[\"host_id\", \"cutoff_month\"]].drop_duplicates()\n",
    "# otw:\n",
    "cutoff_host_mapping = c_scores_subset_f[[\"host_id\", \"cutoff_month\"]].drop_duplicates()\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    cutoff_values, sample_sizes = get_cutoff_value_sample_sizes(\n",
    "        abx_scores_flat, abx_df, n, min_samples, max_samples, group_samples\n",
    "    )\n",
    "    print(f\"Number of cutoffs in abx: {sum(sample_sizes.values())}\")\n",
    "\n",
    "    # subsample in cutoff_host_mapping by sample_sizes:\n",
    "    cutoff_host_mapping_subset = (\n",
    "        cutoff_host_mapping.groupby(\"cutoff_month\", group_keys=False)\n",
    "        .apply(\n",
    "            sample_from_each_group,\n",
    "            sample_sizes=sample_sizes,\n",
    "            seed=8,\n",
    "            include_groups=True,\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"Number of cutoffs in subsample: {cutoff_host_mapping_subset.shape[0]}\")\n",
    "\n",
    "    # VERIFY distributions: Now, cutoff_host_mapping_subset has the same\n",
    "    # cutoff_month distribution as in cutoff_values\n",
    "    display_two_distributions(\n",
    "        orig_values=cutoff_values,\n",
    "        sampled_values=cutoff_host_mapping_subset[\"cutoff_month\"].values,\n",
    "    )\n",
    "\n",
    "    # get only scores from cutoff_host_mapping_subset to plot\n",
    "    sampled_subset_nth = pd.merge(\n",
    "        c_scores_subset_f,\n",
    "        cutoff_host_mapping_subset[[\"host_id\", \"cutoff_month\"]],\n",
    "        on=[\"host_id\", \"cutoff_month\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    assert (\n",
    "        sampled_subset_nth.host_id.nunique()\n",
    "        == cutoff_host_mapping_subset.host_id.nunique()\n",
    "    )\n",
    "    print(f\"UNIQUE HOSTS selected {sampled_subset_nth.host_id.unique()}\")\n",
    "\n",
    "    # if there are multiple scores per months_since_cutoff bin per host - take last\n",
    "    # avoids having multiple scores per host per bin\n",
    "    sampled_subset_nth = select_last_score_per_host_per_bin(sampled_subset_nth)\n",
    "\n",
    "    # group samples prior to cutoff if needed - performed here to ensure same\n",
    "    # seed sampling independent on group_samples value\n",
    "    if group_samples:\n",
    "        group_step, last_bin_indicator = _get_step_n_indicator(max_resolution)\n",
    "        # uniqueness not based on only host_id here - but also on cutoff_month\n",
    "        sampled_subset_nth = _group_samples_prior_to_cutoff(\n",
    "            sampled_subset_nth,\n",
    "            \"months_since_cutoff\",\n",
    "            [\"host_id\", \"cutoff_month\"],\n",
    "            min_samples,\n",
    "            group_step,\n",
    "            last_bin_indicator,\n",
    "        )\n",
    "    else:\n",
    "        last_bin_indicator = min_samples\n",
    "\n",
    "    assert sampled_subset_nth.months_since_cutoff.min() == last_bin_indicator\n",
    "\n",
    "    # plot\n",
    "    _plot_score_after_nth_abx_exposure(\n",
    "        sampled_subset_nth,\n",
    "        x_axis=\"months_since_cutoff\",\n",
    "        y_axis=\"score\",\n",
    "        n=0,\n",
    "        path_to_save=evaluation_path,\n",
    "        flag=f\"noabx_{n}matched_scores\",\n",
    "        tag=f\"with {n}{_get_ordinal_suffix(n)} abx matched cut-offs\",\n",
    "        min_samples=min_samples,\n",
    "        max_samples=max_samples,\n",
    "        max_resolution=max_resolution,\n",
    "        grouped_samples=group_samples,\n",
    "        uniqueness_var_ls=[\"use_obs_until_day\"],\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
