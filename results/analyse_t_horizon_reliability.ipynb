{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to infer reliable time horizon\n",
    "\n",
    "To run this notebook you need to create and activate the following conda environment:\n",
    "\n",
    "```\n",
    "conda create --name score_eval -c conda-forge -c defaults numpy pandas matplotlib seaborn scipy scikit-learn ipython ipykernel -y\n",
    "conda activate score_eval\n",
    "pip install -e .\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.utils_eval_score import (\n",
    "    _get_ordinal_suffix,\n",
    "    _get_step_n_indicator,\n",
    "    _group_samples_prior_to_cutoff,\n",
    "    _plot_score_after_nth_abx_exposure,\n",
    "    get_cutoff_value_sample_sizes,\n",
    "    get_scores_n_abx_info,\n",
    ")\n",
    "from src.utils_t_horizon import (\n",
    "    display_two_distributions,\n",
    "    enrich_scores,\n",
    "    plot_cutoff_date_distribution,\n",
    "    sample_from_each_group,\n",
    "    select_last_score_per_host_per_bin,\n",
    "    transform_cutoff_scores,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# avg. number of days per month\n",
    "DAYS_PER_MONTH = 30.437"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER input: define the inferred model and linked datasets to evaluate here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### USER INPUT START\n",
    "# name of the model\n",
    "model_name = \"saved_models_microbial_novel_alpha_div2/id-55\"\n",
    "# which model version to evaluate: \"best\" or \"last\"\n",
    "point_to_evaluate = \"best\"\n",
    "\n",
    "# name of feature dataset used for model\n",
    "ft_name = \"ft_vat19_anomaly_v20240806_entero_genus\"\n",
    "# name of abx time-series used for model\n",
    "abx_ts_name = \"ts_vat19_abx_v20240806\"\n",
    "\n",
    "# limit evaluation to time range up to this many months (if None no limit is set\n",
    "# and all scores are evaluated)\n",
    "limit_months = 24.0\n",
    "\n",
    "# whether to group samples prior to cutoff in analysis\n",
    "group_samples = True\n",
    "\n",
    "# how many samples prior and after cutoff to consider\n",
    "min_samples = -3.0\n",
    "max_samples = 12.0\n",
    "\n",
    "# whether to filter noabx score samples by having at least 1 obs prior to cutoff\n",
    "no_filter = True\n",
    "\n",
    "# whether to have max. resolution of 0.5 months or not\n",
    "max_resolution = False\n",
    "\n",
    "# scaling factor options:\n",
    "scaling_factors_used = True\n",
    "\n",
    "# if scaling_factors_used is True, then the following options are required:\n",
    "# non-centered = \"nc_std\" or centered = \"std\"\n",
    "stddev_type = \"nc_std\"\n",
    "# moving average window size: 30 or 10\n",
    "moving_avg = 10\n",
    "# whether to include duplicates: \"--RD-True\" or \"\"\n",
    "duplicates = \"--RD-False\"\n",
    "# using lower bound of 1 for SFs: \"lower_bound-1\" or \"\"\n",
    "lower_bound = \"\"\n",
    "\n",
    "#### USER INPUT END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = f\"../data/{model_name}/anomaly_detection/\"\n",
    "\n",
    "res_n_group = f\"g{str(group_samples)[0]}_maxres{str(max_resolution)[0]}\"\n",
    "\n",
    "if scaling_factors_used:\n",
    "    print(\"Scaling factors used.\")\n",
    "    folder_name = f\"using-SF_{stddev_type}_z_scores--moving_avg-{moving_avg}-cummax{lower_bound}{duplicates}\"\n",
    "\n",
    "    cutoff_scores_path = (\n",
    "        f\"{base_path}reliability_eval-val-noabx_best_normal/{folder_name}/\"\n",
    "    )\n",
    "    evaluation_path = f\"{base_path}reliability_evaluation/{res_n_group}_{stddev_type}_ma{moving_avg}{duplicates.replace('-', '_').lower()}/\"\n",
    "\n",
    "    abx_scores_path = f\"{base_path}scores_{point_to_evaluate}_normal/{folder_name}/\"\n",
    "else:\n",
    "    print(\"No scaling factors used.\")\n",
    "    cutoff_scores_path = f\"{base_path}reliability_eval-val-noabx_best_normal/\"\n",
    "    evaluation_path = f\"{base_path}reliability_evaluation/{res_n_group}_no_scaling/\"\n",
    "    abx_scores_path = f\"{base_path}scores_{point_to_evaluate}_normal/\"\n",
    "\n",
    "if not os.path.exists(evaluation_path):\n",
    "    os.makedirs(evaluation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get abx scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noabx_train, noabx_val, abx_scores_flat, abx_df, abx_age_at_all = get_scores_n_abx_info(\n",
    "    abx_scores_path, ft_name, limit_months, abx_ts_name, no_filter=no_filter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get noabx cutoff scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noabx cutoff scores\n",
    "c_scores_list = []\n",
    "i_values = list(range(0, 1141, 30))\n",
    "\n",
    "for i in i_values:\n",
    "    c_scores = pd.read_csv(f\"{cutoff_scores_path}val_noabx_ad_scores_{i}_coord-0.csv\")\n",
    "\n",
    "    # transform scores from wide to long format\n",
    "    c_scores_t = transform_cutoff_scores(c_scores, DAYS_PER_MONTH)\n",
    "\n",
    "    # filter scores\n",
    "    c_scores_t = enrich_scores(c_scores_t, max_resolution=max_resolution)\n",
    "\n",
    "    # append each cutoff date to each other\n",
    "    c_scores_list.append(c_scores_t)\n",
    "\n",
    "c_scores_all = pd.concat(c_scores_list, ignore_index=True)\n",
    "\n",
    "if limit_months is not None:\n",
    "    c_scores_all = c_scores_all[\n",
    "        np.logical_and(\n",
    "            c_scores_all[\"month5_bin\"] <= limit_months,\n",
    "            c_scores_all[\"cutoff_month\"] < limit_months,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "c_scores_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot before filtering:\n",
    "plot_cutoff_date_distribution(c_scores_all, \"- before filtering\")\n",
    "\n",
    "# filter scores\n",
    "if not no_filter:\n",
    "    # filter: at least one observation must be present before the cutoff\n",
    "    c_scores_all_f = c_scores_all[c_scores_all[\"nb_obs_before_cutoff\"] > 0].copy()\n",
    "    # plot after filtering\n",
    "    plot_cutoff_date_distribution(c_scores_all_f, \"- after filtering\")\n",
    "else:\n",
    "    c_scores_all_f = c_scores_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter by months_since_cutoff to be in min_ max_samples range\n",
    "c_scores_subset_f = c_scores_all_f.loc[\n",
    "    np.logical_and(\n",
    "        c_scores_all_f[\"months_since_cutoff\"] >= min_samples,\n",
    "        c_scores_all_f[\"months_since_cutoff\"] <= max_samples,\n",
    "    ),\n",
    "    :,\n",
    "]\n",
    "\n",
    "# bin scores if not max_resolution\n",
    "if not max_resolution:\n",
    "    c_scores_subset_f = c_scores_subset_f.copy()\n",
    "    # bin months_since_cutoff\n",
    "    bins = np.arange(min_samples, max_samples + 1.0, 1.0)\n",
    "    # Use the left edges of the bins as float labels\n",
    "    labels = bins[:-1]\n",
    "    c_scores_subset_f.loc[:, \"months_since_cutoff\"] = pd.cut(\n",
    "        c_scores_subset_f[\"months_since_cutoff\"], bins=bins, labels=labels, right=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all cutoffs grouped by cutoff value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_cutoffs = c_scores_subset_f.use_obs_until_day.unique().tolist()\n",
    "\n",
    "# for cutoff in all_cutoffs:\n",
    "#     c_scores_subset_f_ss = c_scores_subset_f[\n",
    "#         c_scores_subset_f[\"use_obs_until_day\"] == cutoff\n",
    "#     ].copy()\n",
    "#     print(cutoff)\n",
    "\n",
    "#     # avoids having multiple scores per host per bin\n",
    "#     c_scores_subset_f_ss = select_last_score_per_host_per_bin(c_scores_subset_f_ss)\n",
    "\n",
    "#     # group samples prior to cutoff if needed - performed here to ensure same\n",
    "#     # seed sampling independent on group_samples value\n",
    "#     if group_samples:\n",
    "#         group_step, last_bin_indicator = _get_step_n_indicator(max_resolution)\n",
    "#         # uniqueness not based on only host_id here - but also on cutoff_month\n",
    "#         c_scores_subset_f_ss = _group_samples_prior_to_cutoff(\n",
    "#             c_scores_subset_f_ss,\n",
    "#             \"months_since_cutoff\",\n",
    "#             [\"host_id\", \"cutoff_month\"],\n",
    "#             min_samples,\n",
    "#             group_step,\n",
    "#             last_bin_indicator,\n",
    "#         )\n",
    "\n",
    "#     _plot_score_after_nth_abx_exposure(\n",
    "#         c_scores_subset_f_ss,\n",
    "#         x_axis=\"months_since_cutoff\",\n",
    "#         y_axis=\"score\",\n",
    "#         n=0,\n",
    "#         path_to_save=evaluation_path,\n",
    "#         flag=f\"noabx_cutoff{int(cutoff)}_scores\",\n",
    "#         tag=f\"with cutoff={cutoff}\",\n",
    "#         min_samples=min_samples,\n",
    "#         max_samples=max_samples,\n",
    "#         max_resolution=max_resolution,\n",
    "#         grouped_samples=group_samples,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize all cutoffs pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoids having multiple scores per host per bin\n",
    "c_scores_subset_f = select_last_score_per_host_per_bin(c_scores_subset_f)\n",
    "\n",
    "# group samples prior to cutoff if needed - performed here to ensure same\n",
    "# seed sampling independent on group_samples value\n",
    "if group_samples:\n",
    "    group_step, last_bin_indicator = _get_step_n_indicator(max_resolution)\n",
    "    # uniqueness not based on only host_id here - but also on cutoff_month\n",
    "    c_scores_subset_f = _group_samples_prior_to_cutoff(\n",
    "        c_scores_subset_f,\n",
    "        \"months_since_cutoff\",\n",
    "        [\"host_id\", \"cutoff_month\"],\n",
    "        min_samples,\n",
    "        group_step,\n",
    "        last_bin_indicator,\n",
    "    )\n",
    "\n",
    "\n",
    "_plot_score_after_nth_abx_exposure(\n",
    "    c_scores_subset_f,\n",
    "    x_axis=\"months_since_cutoff\",\n",
    "    y_axis=\"score\",\n",
    "    n=0,\n",
    "    path_to_save=evaluation_path,\n",
    "    flag=\"noabx_cutoff_all_scores\",\n",
    "    tag=\"\",\n",
    "    min_samples=min_samples,\n",
    "    max_samples=max_samples,\n",
    "    max_resolution=max_resolution,\n",
    "    grouped_samples=group_samples,\n",
    "    uniqueness_var_ls=[\"use_obs_until_day\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize cutoffs - same \"cutoff\" subset as for 1st/2nd/3rd abx scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max_samples to 6 for consistency with abx comparison in evaluate_scores\n",
    "max_samples = 6.0\n",
    "\n",
    "# filter by months_since_cutoff to be in min_ max_samples range\n",
    "c_scores_subset_f = c_scores_all_f.loc[\n",
    "    np.logical_and(\n",
    "        c_scores_all_f[\"months_since_cutoff\"] >= min_samples,\n",
    "        c_scores_all_f[\"months_since_cutoff\"] <= max_samples,\n",
    "    ),\n",
    "    :,\n",
    "]\n",
    "\n",
    "# bin scores if not max_resolution\n",
    "if not max_resolution:\n",
    "    c_scores_subset_f = c_scores_subset_f.copy()\n",
    "    # bin months_since_cutoff\n",
    "    bins = np.arange(min_samples, max_samples + 1.0, 1.0)\n",
    "    # Use the left edges of the bins as float labels\n",
    "    labels = bins[:-1]\n",
    "    c_scores_subset_f.loc[:, \"months_since_cutoff\"] = pd.cut(\n",
    "        c_scores_subset_f[\"months_since_cutoff\"], bins=bins, labels=labels, right=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all host's cutoffs to choose from\n",
    "\n",
    "# # to only take hosts that have a value after cutoff\n",
    "# c_scores_subset_f_f = c_scores_subset_f[c_scores_subset_f[\"months_since_cutoff\"]==0.0].copy()\n",
    "# cutoff_host_mapping = c_scores_subset_f_f[[\"host_id\", \"cutoff_month\"]].drop_duplicates()\n",
    "# otw:\n",
    "cutoff_host_mapping = c_scores_subset_f[[\"host_id\", \"cutoff_month\"]].drop_duplicates()\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    cutoff_values, sample_sizes = get_cutoff_value_sample_sizes(\n",
    "        abx_scores_flat, abx_df, n, min_samples, max_samples, group_samples\n",
    "    )\n",
    "    print(f\"Number of cutoffs in abx: {sum(sample_sizes.values())}\")\n",
    "\n",
    "    # subsample in cutoff_host_mapping by sample_sizes:\n",
    "    cutoff_host_mapping_subset = (\n",
    "        cutoff_host_mapping.groupby(\"cutoff_month\", group_keys=False)\n",
    "        .apply(\n",
    "            sample_from_each_group,\n",
    "            sample_sizes=sample_sizes,\n",
    "            seed=8,\n",
    "            include_groups=True,\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    print(f\"Number of cutoffs in subsample: {cutoff_host_mapping_subset.shape[0]}\")\n",
    "\n",
    "    # VERIFY distributions: Now, cutoff_host_mapping_subset has the same\n",
    "    # cutoff_month distribution as in cutoff_values\n",
    "    display_two_distributions(\n",
    "        orig_values=cutoff_values,\n",
    "        sampled_values=cutoff_host_mapping_subset[\"cutoff_month\"].values,\n",
    "    )\n",
    "\n",
    "    # get only scores from cutoff_host_mapping_subset to plot\n",
    "    sampled_subset_nth = pd.merge(\n",
    "        c_scores_subset_f,\n",
    "        cutoff_host_mapping_subset[[\"host_id\", \"cutoff_month\"]],\n",
    "        on=[\"host_id\", \"cutoff_month\"],\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    assert (\n",
    "        sampled_subset_nth.host_id.nunique()\n",
    "        == cutoff_host_mapping_subset.host_id.nunique()\n",
    "    )\n",
    "    print(f\"UNIQUE HOSTS selected {sampled_subset_nth.host_id.unique()}\")\n",
    "\n",
    "    # if there are multiple scores per months_since_cutoff bin per host - take last\n",
    "    # avoids having multiple scores per host per bin\n",
    "    sampled_subset_nth = select_last_score_per_host_per_bin(sampled_subset_nth)\n",
    "\n",
    "    # group samples prior to cutoff if needed - performed here to ensure same\n",
    "    # seed sampling independent on group_samples value\n",
    "    if group_samples:\n",
    "        group_step, last_bin_indicator = _get_step_n_indicator(max_resolution)\n",
    "        # uniqueness not based on only host_id here - but also on cutoff_month\n",
    "        sampled_subset_nth = _group_samples_prior_to_cutoff(\n",
    "            sampled_subset_nth,\n",
    "            \"months_since_cutoff\",\n",
    "            [\"host_id\", \"cutoff_month\"],\n",
    "            min_samples,\n",
    "            group_step,\n",
    "            last_bin_indicator,\n",
    "        )\n",
    "    else:\n",
    "        last_bin_indicator = min_samples\n",
    "\n",
    "    assert sampled_subset_nth.months_since_cutoff.min() == last_bin_indicator\n",
    "\n",
    "    # plot\n",
    "    _plot_score_after_nth_abx_exposure(\n",
    "        sampled_subset_nth,\n",
    "        x_axis=\"months_since_cutoff\",\n",
    "        y_axis=\"score\",\n",
    "        n=0,\n",
    "        path_to_save=evaluation_path,\n",
    "        flag=f\"noabx_{n}matched_scores\",\n",
    "        tag=f\"with {n}{_get_ordinal_suffix(n)} abx matched cut-offs\",\n",
    "        min_samples=min_samples,\n",
    "        max_samples=max_samples,\n",
    "        max_resolution=max_resolution,\n",
    "        grouped_samples=group_samples,\n",
    "        uniqueness_var_ls=[\"use_obs_until_day\"],\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "score_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
